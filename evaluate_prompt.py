from codecarbon import EmissionsTracker
import ollama

tracker = EmissionsTracker(project_name="deepseek_test")

tracker.start()

# Anfrage an DeepSeek (ersetze ggf. Modellnamen)
response = ollama.chat(
    model='deepseek-r1:latest',
    messages=[
        {'role': 'user', 'content': 'Consider the following Prompt: Can you help me write an application that can parse audio files for human speech, and remove sounds that fall into common english dipthongs? Consider the following answer:'
        'Antwort: To create an application that removes specific English diphthong sounds from audio files, you\'ll need to combine **audio processing** (to isolate frequencies or segments) with **speech recognition** (to analyze phonetic content). Here\'s a structured approach using Python and relevant libraries:\n\n---\n\n### Step 1: Install Required Libraries\n```bash\npip install librosa numpy scipy vosk speech_recognition sounddevice pydub pandas\n```\n\n#### Key Libraries:\n- `librosa`: Audio analysis.\n- `vosk`: Speech recognition (offline).\n- `sounddevice`/`scipy`: Playback/output handling.\n\n---\n\n### Step 2: Identify Diphthong Frequencies\nEnglish diphthongs have formant frequencies that vary, but here are typical ranges:\n| Diphthong | F1 Range (Hz) | F2 Range (Hz) |\n|-----------|--------------|--------------|\n| /aʊ/ (ou)   | 300–600       | 200–700      |\n| /eɪ/ (ai)   | 400–850       | —            |\n| /ɪə/ (ear)  | 310–690       | 200–800      |\n\nUse a pre-trained model or spectral analysis to detect these frequencies in the audio.\n\n---\n\n### Step 3: Audio Processing Workflow\n#### A. Load and Preprocess Audio\n```python\nimport librosa\n\ndef load_audio(file_path):\n    y, sr = librosa.load(file_path, sr=None)  # Preserve sample rate\n    return y, sr\n\n# Example usage:\naudio_data, sample_rate = load_audio("input.wav")\n```\n\n#### B. Remove Diphthong Frequencies (Using Bandpass Filtering)\n```python\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\n\ndef remove_diphthongs(y, diphthong_ranges):\n    """Apply bandstop filters to remove specified frequency ranges."""\n    for f1_min, f1_max in diphthong_ranges:\n        # Design a Butterworth bandstop filter (order=5)\n        nyquist = 0.5 * sample_rate\n        low = f1_min / nyquist\n        high = f1_max / nyquist\n        b, a = butter(5, [low, high], btype=\'bandstop\')\n        y = filtfilt(b, a, y)  # Apply filter to the signal\n    return y\n\n# Example usage:\ndiphthong_ranges = [[300, 800]]  # Target frequencies for /aʊ/ and others\nfiltered_audio = remove_diphthongs(audio_data, diphthong_ranges)\n```\n\n#### C. Speech Recognition (Optional: Verify Diphthong Removal)\n```python\nimport vosk\n\n# Initialize Vosk speech recognizer\nmodel = vosk.Model("vosk-model-en-us-0.22")  # Download the model first!\nrec = vosk.KaldiRecognizer(model, sample_rate)\n\ndef transcribe(audio_path):\n    """Transcribe audio to text and identify diphthong occurrences."""\n    # Use Vosk to recognize phonemes (optional)\n    # This requires advanced setup with acoustic models\n\n# Play back filtered_audio using sounddevice\n```\n\n---\n\n### Step 4: Save the Processed Audio\n```python\nfrom pydub import AudioSegment\n\ndef save_audio(y, sample_rate, output_path):\n    """Convert numpy array to WAV and save."""\n    # Convert to mono (if stereo)\n    y_mono = librosa.to_mono(y) if len(y.shape) > 1 else y\n    y_resampled = librosa.resample(y_mono, sample_rate, target_sr=44100)  # Adjust as needed\n    output = AudioSegment(\n        (y_resampled * 32768).astype(np.int16), \n        frame_rate=44100,\n        channels=1\n    )\n    output.export(output_path, format="wav")\n\nsave_audio(filtered_audio, sample_rate, "output.wav")\n```\n\n---\n\n### Step 5: User Interface (Optional)\nFor a GUI-based tool, use `PyQt5` or `Tkinter`. Here’s a minimal PyQt example:\n\n```python\nfrom PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QFileDialog\n\nclass AudioEditor(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.initUI()\n\n    def initUI(self):\n        button = QPushButton("Process File", self)\n        button.clicked.connect(self.process_file)\n        self.setGeometry(300, 300, 400, 200)\n        self.show()\n\n    def process_file(self):\n        file_path, _ = QFileDialog.getOpenFileName(None, "Select Audio File")\n        # Call your audio processing functions here\n\nif __name__ == "__main__":\n    app = QApplication([])\n    editor = AudioEditor()\n    sys.exit(app.exec_())\n```\n\n---\n\n### Notes:\n1. **Bandpass filtering** removes frequencies but may distort speech.\n2. Use a more sophisticated approach (e.g., machine learning) if you want to selectively target diphthong sounds while preserving other vocal elements.\n3. Adjust frequency ranges based on your audio analysis results.\n\n---\n\n### Example Full Code\n```python\nimport sys\nimport librosa\nfrom pydub import AudioSegment\n\n# Load settings from config file (if desired)\n\ndef butter_bandstop_filter(data, sample_rate, lowcut=500, highcut=1200):\n    """Bandstop filter to remove diphthong frequencies."""\n    nyquist = 0.5 * sample_rate\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(3, [low, high], btype=\'bandstop\')\n    return filtfilt(b, a, data)\n\n# User inputs file path and diphthong ranges via GUI/file dialog\ninput_file, output_file = get_paths_from_ui()\nprocess_audio(input_file, output_file)\n```\n\n---\n\n### Limitations:\n- **Accuracy**: Bandpass filtering might remove unintended frequencies.\n- **Complexity**: Distinguishing diphthongs from other sounds requires advanced acoustic models (e.g., using Praat or custom Vosk training).\n- **Speech Recognition Integration**: Requires additional work to map audio segments to phonemes.\n\n---\n\n### Advanced Option: Use a Speech-to-Text API\nFor more accuracy, consider cloud services like Google Cloud Speech-to-Text. However, this requires an internet connection and paid plans for streaming APIs.\n\n---\n\nLet me know if you’d like help with:\n1. **Diphthong detection code** using spectral features (e.g., MFCCs).\n2. **GUI integration**.\n3. Handling stereo audio or variable input formats.'
        'Give a rating from 1 to 10 how good the answer answers the prompt.'
        }
    ]
)

tracker.stop()

print("Antwort:", response['message']['content'])